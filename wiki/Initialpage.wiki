https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo3.png

= Social Robotics using NAO and Kinect =

== Robotics Project - Interaction Lab - Heriot-Watt University ==

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo2.png

== Members == 

== Contents ==

 # [#Introduction Introduction]
 # [#Problem_Statement Problem Statement]
 # [#Method Method]
  * [Preprocessing]
  * [Matchingdistance 2D Chamfer Matching Distance]
  * [HeadParameters Head Parameters Estimation]
  * [Segmentation]
  * [Tracking]
 # [#Implementation Implementation]
  * [SoftwareArchitecture Software Architecture]
 # [#Results Results]
 # [#Conclusions Conclusions]

== Introduction ==

As robots become involved into daily life, they must need to deal under situations in which social interaction is essential. It implies robot must be able to satisfy the social goals and obligations that come up through interactions with people in real-world settings, which demand challenges on reasoning, decision making and action selection components of the system.

A social robot is defined as "an autonomous robot that interacts and communicates with humans or other autonomous physical agents by following social behaviours and rules attached to its role" Additionally, social robots need to understand and react intelligently to the actions and intentions of multiple humans in a visual scene.

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/socialrobots.png

This project combines a NAO torso with a Kinect controller in order to estimate the social scene in a bar/pub/caf√© style interaction with multiple users. The main work in the project will be in vision by using the Kinect to detect multiple humans and perform face tracking and gaze estimation. A particular focus will be on detecting when humans want attention from the robot.

== Problem Statement ==

Humans use head pose and gaze direction as nonverbal cues to express communicative acts and their visual focus of attention. Additionally, head direction and pointing gestures can contribute significantly to the computation of the attention of an interaction partner. 

Hence, a combination of head pose estimation and recognition of gaze directions is needed to exactly determine the visual attention of an interaction partner.  Gaschler et al. show in [2] that head pose is an important cue that is used by customers and bartenders in all steps of the ordering sequence. 

Humans can easily estimate the head pose of an interaction partner, but for a computer vision system this ability is rather difficult to achieve. The pose is described by the approximate position of the head in space, and an angle that describes torso orientation.

Many methods has been used to generate a head pose estimation. Appearance-based methods consider the image region of the face as a whole, while Feature-based methods extract from the actual region and recover low-dimensional features (position of facial features). Both techniques can be combined in order to track facial features.

== Method ==
 * [Preprocessing]
 * [Matchingdistance 2D Chamfer Matching Distance]
 * [HeadParameters Head Parameters Estimation]
 * [Segmentation]
 * [Tracking]
== Results ==
== Implementation ==
 * [SoftwareArchitecture Software Architecture]
== Conclusions ==