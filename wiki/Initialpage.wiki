https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo3.png

= Social Robotics using NAO and Kinect =

== Robotics Project - Interaction Lab - Heriot-Watt University ==

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo2.png

== Members == 

== Contents ==

 # [#Introduction Introduction]
 # [#Problem_Statement Problem Statement]
 # [#Method Method]
  * [Preprocessing]
  * [Matchingdistance 2D Chamfer Matching Distance]
  * [HeadParameters Head Parameters Estimation]
  * [TorsoOrientation Torso Orientation]
  * [Tracking]
 # [#Implementation Implementation]
  * [Kinect Getting Started with Kinect]
  * [SoftwareArchitecture Software Architecture]
  * [http://scoialrobot.googlecode.com/svn/trunk/social_robot/documentation/html/index.html Documentation]
 # [#Demo Demo]
 # [#Conclusions Conclusions]
 # [References References]
 # [#Links_of_Interest Links of Interest]

== Introduction ==

As robots become involved into daily life, they must need to deal under situations in which social interaction is essential. It implies robot must be able to satisfy the social goals and obligations that come up through interactions with people in real-world settings. [References 1] Usually, these interactions demand challenges on reasoning, decision making and action selection components of the system.

A social robot is defined as "an autonomous robot that interacts and communicates with humans or other autonomous physical agents by following social behaviours and rules attached to its role" Additionally, social robots need to understand and react intelligently to the actions and intentions of multiple humans in a visual scene.

One of the most advanced research groups, the [http://web.media.mit.edu/~cynthiab/research/research.html Personal Robots Group from  Massachusetts Institute of Technology] develops techniques and technologies for social robot applications. The figure below shows four important projects: [http://www.youtube.com/watch?v=ilmDN2e_Flc&feature=related Leonardo Robot], [http://www.youtube.com/watch?v=wGowILhHCCo the sociable robot car AIDA],[http://www.youtube.com/watch?v=JxmZyEH4IVI TOFU], and [http://www.youtube.com/watch?v=aQS2zxmrrrA the Mobile, Dexterous, Social Robot]

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/socialrobots.png

Detecting people in images or videos represents a challenging problem due to changes in pose, clothing, lighting conditions and backgrounds. Some methods oriented to the human detection involve statistical training based on local features, i.e. gradient-based features such as HOG; and some involve extracting interest points in the image, such as scale-invariant feature transform (SIFT). [References 2]

Depth information is an important cue that can be found in range images.  These have several advantages over 2D intensity images. For instance, range images are robust to the changes in color and illumination; and offer a simple representation of 3D information. The first range sensors were expensive and difficult to use in human environments because of lasers. The develop of depth sensors, such as Kinect from Microsoft, has allowed the application of range sensors in human environment, and hence, the encouragement of research in human detection, tracking and activity analysis.

This project combines a NAO torso with a Kinect controller in order to estimate the social scene in a bar/pub/caf√© style interaction with multiple users. The main work in the project will be in vision by using the Kinect to detect multiple humans and perform face tracking and gaze estimation. A particular focus will be on detecting when humans want attention from the robot.

== Problem Statement ==

Humans use head pose and gaze direction as nonverbal cues to express communicative acts and their visual focus of attention. Additionally, head direction and pointing gestures can contribute significantly to the computation of the attention of an interaction partner. 

A combination of head pose estimation and recognition of gaze directions is needed to exactly determine the visual attention of an interaction partner.  Gaschler et al. show in [References 3] that head pose is an important cue that is used by customers and bartenders in all steps of the ordering sequence. 

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/bar_scene.jpg

Humans can easily estimate the head pose of an interaction partner, but for a computer vision system this ability is rather difficult to achieve. The pose is described by the approximate position of the head in space, and an angle that describes torso orientation.

Many methods have been used to generate a head pose estimation. Appearance-based methods consider the image region of the face as a whole, while Feature-based methods extract from the actual region and recover low-dimensional features (position of facial features). Both techniques can be combined in order to track facial features.

== Method ==
 * [Preprocessing]
 * [Matchingdistance 2D Chamfer Matching Distance]
 * [HeadParameters Head Parameters Estimation]
 * [TorsoOrientation Torso Orientation]
 * [Tracking]
== Demo ==
== Implementation ==
 * [Kinect Getting Started with Kinect]
 * [SoftwareArchitecture Software Architecture]
 * [http://scoialrobot.googlecode.com/svn/trunk/social_robot/documentation/html/index.html Documentation]
== Conclusions ==


== Links of Interest == 
*Presentations*
 # [http://prezi.com/e-5801a4gmsm/rp-presentation-1/ Presentation 1 -  October 17th, 2012]
 # [http://prezi.com/qvdukdcb9ikg/rp-presentation-2/ Presentation 2 -  October 30th, 2012]

*Dataset and Demo for Human Detections*
 # [http://www.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html RGB-D People Dataset]
 # [http://www.youtube.com/watch?feature=player_embedded&v=UwHOGfXxajM People Detection in RGB-D Data]

*Projects, Demos and Group Research*
 # [http://www.james-project.eu EU project JAMES]
 # [https://sites.google.com/site/hwinteractionlab/home Interaction Lab - Heriot-Watt University]
 # [http://www.youtube.com/watch?v=8k7Pd-CbbhE&feature=youtu.be JAMES, the robot bartender, system evaluation April / May 2012]