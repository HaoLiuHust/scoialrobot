#summary Explaining how ros topics work with each other.

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo3.png

= Introduction =

In this page we're going to explain how different ROS topics work with each other in our application. In addition to this page we have created Doxygen documentation for our source code, which is available [http://scoialrobot.googlecode.com/svn/trunk/social_robot/documentation/html/index.html here].

= Overview =

The high level component diagram of our application can be observed in figure below. Kinect publishes three different topics, RGB, disparity and depth images. The details of these topics can be read in [http://www.ros.org/wiki/openni_launch OpenNI Lunch Package].

We have used two independent detectors, RGB and depth. Therefore, RGB detector subscribes to the RGB images. And depth detector subscribes to the depth and disparity images. Each of these detector perform their tasks and publish their data.

Tracking component subscribes to both detectors. It performs its tracking assignment and publishes the result of tracking, which is ultimately the result of our application.

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/flowchart_diagram.png

The ROS topic communication can be observed in figure below:

https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/rostopics.png

= Low level description =

In this section we are going to explain in detail how each component exactly works.

== Kinect == 

We first have to run the OpenNI Launch as follow:

{{{
  roslaunch openni_launch openni.launch
}}}

Since we're interested in the depth registered images we set the depth registration parameter as true, this is done in c++ code as follow:

{{{
  ros::NodeHandle nh;
  // to register the depth
  nh.setParam ( "/camera/driver/depth_registration", true );
}}}

[http://www.ros.org/wiki/openni_launch OpenNI Lunch Package] publishes many topics, we're interested in mainly three of them:
 * camera/rgb/image_color
 * camera/depth_registered/image_raw
 * camera/depth_registered/disparity

== RGB Detector ==

RGB detector subscribes to _camera/rgb/image_color_ which publishes [http://www.ros.org/doc/api/sensor_msgs/html/msg/Image.html sensor_msgs/Image]. We do perform detection every _n_ frame. _n_ can be updated dynamically during the run time, by calling _rosparam set_.

For detection we're using OpenCV [http://docs.opencv.org/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html#cascade-classifier Cascade Classifier]. We are using [http://scoialrobot.googlecode.com/svn/trunk/social_robot/rsrc/haarcascades/haarcascade_frontalface_alt.xml Front Face Alt] classifier. For a list of other OpenCV already made classifiers please check [http://scoialrobot.googlecode.com/svn/trunk/social_robot/rsrc/haarcascades/ here].

RGB detector publishes its detected faces under _/social_robot/rgb/rois_ ([http://scoialrobot.googlecode.com/svn/trunk/social_robot/msg/RegionOfInterests.msg RegionOfInterests]) which is  basically list of [http://www.ros.org/doc/api/sensor_msgs/html/msg/RegionOfInterest.html sensor_msgs/RegionOfInterest].

== Depth Detector ==

Depth detector subscribes to  _camera/depth_registered/image_raw_ ([http://www.ros.org/doc/api/sensor_msgs/html/msg/Image.html sensor_msgs/Image]) and _camera/depth_registered/disparity_ ([http://www.ros.org/doc/api/stereo_msgs/html/msg/DisparityImage.html stereo_msgs/DisparityImage]). Similar to the RGB detector we perform detection in every _n_ frame. And _n_ can be changed dynamically during runtime.

The algorithm for this detection is basically implemented in [http://scoialrobot.googlecode.com/svn/trunk/social_robot/src/DepthFaceDetector.h DepthFaceDetector.h] and [http://scoialrobot.googlecode.com/svn/trunk/social_robot/src/DepthFaceDetector.c  DepthFaceDetector.c]. The explanation of this algorithm can be found in our [https://code.google.com/p/scoialrobot/wiki/Initialpage#Method wiki pages] as well as [References 3].

The result of detected faces in depth are published in topic _/social_robot/depth/rois_ ([http://scoialrobot.googlecode.com/svn/trunk/social_robot/msg/RegionOfInterests.msg RegionOfInterests]) which is  basically list of [http://www.ros.org/doc/api/sensor_msgs/html/msg/RegionOfInterest.html sensor_msgs/RegionOfInterest].

== Tracking ==

This module can be run as an standalone application, in which both detection and tracking is performed, however this is not advised because of computational time. That's why this module can also be run only as tracking module in which it subscribes to both the detected faces by RGB and depth.

For tracking we're performing particle filter, explained in [Tracking tracking] page of our wiki. The result of this module is published under _/social_robot/track/rois_ ([http://scoialrobot.googlecode.com/svn/trunk/social_robot/msg/RegionOfInterests.msg RegionOfInterests]) which is  basically list of [http://www.ros.org/doc/api/sensor_msgs/html/msg/RegionOfInterest.html sensor_msgs/RegionOfInterest].

== User interface ==

We have implemented a simple QT application, in which user can select the view change the parameters, etc. The intention is mainly for testing purposes.

= Rationale =

We have tried to achieve many quality attributes in our software architecture.

== Modularity ==

The architecture of our programme is designed in a manner which makes them as modular as possible. This means each topic has a specific task to do. This makes it very usable for any other usage, e.g. if someone just to use our face detector in RGB or depth independently they can simply do so.

Additionally, new components can be easily added to the system and current components can easily replaced. The architecture is designed in a low-couple manner, this means simply you can plug-in new modules.

== Robustness ==

If one components stops working, let's say the depth detector, other components can continue to perform their tasks. This means crashing in one area doesn't influence the entire system.

== Parallelism ==

Detection and tracking are both very heavy computationally speaking. With our design each of these modules are run in one thread and therefore the speed is much better. This also gives the opportunity to work in a distributed system. Detections can happen from multiple on-line cameras and send the results to tracking.